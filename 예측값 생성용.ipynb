{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"예측값 생성용.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNCVsqCLN8stwN+nw5C5mig"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"GlJyQiXfnG8S"},"source":["!pip install transformers\r\n","!pip install SentencePiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQRLghn0nIBV"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UerHJ-dOnHkJ"},"source":["#경로 설정\r\n","import os\r\n","os.chdir('/content/drive/My Drive/Colab Notebooks/AI야진짜뉴스를찾아줘')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYXJbUY1mw69"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5WxfAVsnE3w"},"source":["train = pd.read_csv(\"news_train.csv\")\r\n","test = pd.read_csv(\"news_test.csv\")\r\n","sample_submission = pd.read_csv(\"sample_submission.csv\", encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jaYM_alpnbQ5"},"source":["## 시간 측정 시작"]},{"cell_type":"code","metadata":{"id":"NKL61rzvYDWs"},"source":["import time\r\n","start = time.time()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N645strrnE1P"},"source":["# from tensorflow.keras.preprocessing import text, sequence \r\n","# from tensorflow.keras.models import Sequential\r\n","\r\n","from keras.models import Model\r\n","from keras import models, layers\r\n","from tensorflow.keras.layers import Input, Dense, Embedding, Dropout, concatenate\r\n","# from keras.layers import Bidirectional, SpatialDropout1D, MaxPooling1D, GlobalMaxPooling1D\r\n","# from keras.preprocessing.text import Tokenizer\r\n","# from keras.preprocessing.sequence import pad_sequences\r\n","\r\n","import tensorflow as tf\r\n","import numpy as np\r\n","import pandas as pd\r\n","from transformers import *\r\n","from tqdm import tqdm\r\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dc1sos1XY-E"},"source":["SEQ_LEN = 64\r\n","BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cc5EKxxAnEyf"},"source":["train = train.drop(['n_id', 'date', 'ord'],axis=1)\r\n","# train = train.reset_index(drop=True)\r\n","\r\n","test = test.drop(['n_id', 'date', 'ord', 'id'],axis=1)\r\n","# test = test.reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NVRSB-WPWiuV"},"source":["def clean_text(texts):\r\n","    corpus = []\r\n","    for i in range(0, len(texts)):\r\n","        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\r\n","        review = re.sub(r'\\d+','', str(texts[i]))# remove number\r\n","        # review = review.lower() #lower case\r\n","        review = re.sub(r'\\s+', ' ', review) #remove extra space\r\n","        review = re.sub(r'<[^>]+>','',review) #remove Html tags\r\n","        review = re.sub(r'\\s+', ' ', review) #remove spaces\r\n","        review = re.sub(r\"^\\s+\", '', review) #remove space from start\r\n","        review = re.sub(r'\\s+$', '', review) #remove space from the end\r\n","        corpus.append(review)\r\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYyosD4bWnb1"},"source":["test.content = clean_text(test.content)\r\n","test.title = clean_text(test.title)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"brXjJZeNnEv8"},"source":["# coding=utf-8\r\n","# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team and Jangwon Park\r\n","#\r\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n","# you may not use this file except in compliance with the License.\r\n","# You may obtain a copy of the License at\r\n","#\r\n","#     http://www.apache.org/licenses/LICENSE-2.0\r\n","#\r\n","# Unless required by applicable law or agreed to in writing, software\r\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n","# See the License for the specific language governing permissions and\r\n","# limitations under the License.\r\n","\"\"\" Tokenization classes for KoBert model.\"\"\"\r\n","\r\n","\r\n","import logging\r\n","import os\r\n","import unicodedata\r\n","from shutil import copyfile\r\n","\r\n","from transformers import PreTrainedTokenizer\r\n","\r\n","\r\n","logger = logging.getLogger(__name__)\r\n","\r\n","VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\r\n","                     \"vocab_txt\": \"vocab.txt\"}\r\n","\r\n","PRETRAINED_VOCAB_FILES_MAP = {\r\n","    \"vocab_file\": {\r\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\r\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\r\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\r\n","    },\r\n","    \"vocab_txt\": {\r\n","        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\r\n","        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\r\n","        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\r\n","    }\r\n","}\r\n","\r\n","PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\r\n","    \"monologg/kobert\": 512,\r\n","    \"monologg/kobert-lm\": 512,\r\n","    \"monologg/distilkobert\": 512\r\n","}\r\n","\r\n","PRETRAINED_INIT_CONFIGURATION = {\r\n","    \"monologg/kobert\": {\"do_lower_case\": False},\r\n","    \"monologg/kobert-lm\": {\"do_lower_case\": False},\r\n","    \"monologg/distilkobert\": {\"do_lower_case\": False}\r\n","}\r\n","\r\n","SPIECE_UNDERLINE = u'▁'\r\n","\r\n","\r\n","class KoBertTokenizer(PreTrainedTokenizer):\r\n","    \"\"\"\r\n","        SentencePiece based tokenizer. Peculiarities:\r\n","            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\r\n","    \"\"\"\r\n","    vocab_files_names = VOCAB_FILES_NAMES\r\n","    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\r\n","    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\r\n","    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\r\n","\r\n","    def __init__(\r\n","            self,\r\n","            vocab_file,\r\n","            vocab_txt,\r\n","            do_lower_case=False,\r\n","            remove_space=True,\r\n","            keep_accents=False,\r\n","            unk_token=\"[UNK]\",\r\n","            sep_token=\"[SEP]\",\r\n","            pad_token=\"[PAD]\",\r\n","            cls_token=\"[CLS]\",\r\n","            mask_token=\"[MASK]\",\r\n","            **kwargs):\r\n","        super().__init__(\r\n","            unk_token=unk_token,\r\n","            sep_token=sep_token,\r\n","            pad_token=pad_token,\r\n","            cls_token=cls_token,\r\n","            mask_token=mask_token,\r\n","            **kwargs\r\n","        )\r\n","\r\n","        # Build vocab\r\n","        self.token2idx = dict()\r\n","        self.idx2token = []\r\n","        with open(vocab_txt, 'r', encoding='utf-8') as f:\r\n","            for idx, token in enumerate(f):\r\n","                token = token.strip()\r\n","                self.token2idx[token] = idx\r\n","                self.idx2token.append(token)\r\n","\r\n","        try:\r\n","            import sentencepiece as spm\r\n","        except ImportError:\r\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n","                           \"pip install sentencepiece\")\r\n","\r\n","        self.do_lower_case = do_lower_case\r\n","        self.remove_space = remove_space\r\n","        self.keep_accents = keep_accents\r\n","        self.vocab_file = vocab_file\r\n","        self.vocab_txt = vocab_txt\r\n","\r\n","        self.sp_model = spm.SentencePieceProcessor()\r\n","        self.sp_model.Load(vocab_file)\r\n","\r\n","    @property\r\n","    def vocab_size(self):\r\n","        return len(self.idx2token)\r\n","\r\n","    def get_vocab(self):\r\n","        return dict(self.token2idx, **self.added_tokens_encoder)\r\n","\r\n","    def __getstate__(self):\r\n","        state = self.__dict__.copy()\r\n","        state[\"sp_model\"] = None\r\n","        return state\r\n","\r\n","    def __setstate__(self, d):\r\n","        self.__dict__ = d\r\n","        try:\r\n","            import sentencepiece as spm\r\n","        except ImportError:\r\n","            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\r\n","                           \"pip install sentencepiece\")\r\n","        self.sp_model = spm.SentencePieceProcessor()\r\n","        self.sp_model.Load(self.vocab_file)\r\n","\r\n","    def preprocess_text(self, inputs):\r\n","        if self.remove_space:\r\n","            outputs = \" \".join(inputs.strip().split())\r\n","        else:\r\n","            outputs = inputs\r\n","        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\r\n","\r\n","        if not self.keep_accents:\r\n","            outputs = unicodedata.normalize('NFKD', outputs)\r\n","            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\r\n","        if self.do_lower_case:\r\n","            outputs = outputs.lower()\r\n","\r\n","        return outputs\r\n","\r\n","    def _tokenize(self, text, return_unicode=True, sample=False):\r\n","        \"\"\" Tokenize a string. \"\"\"\r\n","        text = self.preprocess_text(text)\r\n","\r\n","        if not sample:\r\n","            pieces = self.sp_model.EncodeAsPieces(text)\r\n","        else:\r\n","            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\r\n","        new_pieces = []\r\n","        for piece in pieces:\r\n","            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\r\n","                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\r\n","                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\r\n","                    if len(cur_pieces[0]) == 1:\r\n","                        cur_pieces = cur_pieces[1:]\r\n","                    else:\r\n","                        cur_pieces[0] = cur_pieces[0][1:]\r\n","                cur_pieces.append(piece[-1])\r\n","                new_pieces.extend(cur_pieces)\r\n","            else:\r\n","                new_pieces.append(piece)\r\n","\r\n","        return new_pieces\r\n","\r\n","    def _convert_token_to_id(self, token):\r\n","        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\r\n","        return self.token2idx.get(token, self.token2idx[self.unk_token])\r\n","\r\n","    def _convert_id_to_token(self, index, return_unicode=True):\r\n","        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\r\n","        return self.idx2token[index]\r\n","\r\n","    def convert_tokens_to_string(self, tokens):\r\n","        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\r\n","        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\r\n","        return out_string\r\n","\r\n","    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\r\n","        \"\"\"\r\n","        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\r\n","        by concatenating and adding special tokens.\r\n","        A KoBERT sequence has the following format:\r\n","            single sequence: [CLS] X [SEP]\r\n","            pair of sequences: [CLS] A [SEP] B [SEP]\r\n","        \"\"\"\r\n","        if token_ids_1 is None:\r\n","            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\r\n","        cls = [self.cls_token_id]\r\n","        sep = [self.sep_token_id]\r\n","        return cls + token_ids_0 + sep + token_ids_1 + sep\r\n","\r\n","    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\r\n","        \"\"\"\r\n","        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\r\n","        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\r\n","        Args:\r\n","            token_ids_0: list of ids (must not contain special tokens)\r\n","            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\r\n","                for sequence pairs\r\n","            already_has_special_tokens: (default False) Set to True if the token list is already formated with\r\n","                special tokens for the model\r\n","        Returns:\r\n","            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\r\n","        \"\"\"\r\n","\r\n","        if already_has_special_tokens:\r\n","            if token_ids_1 is not None:\r\n","                raise ValueError(\r\n","                    \"You should not supply a second sequence if the provided sequence of \"\r\n","                    \"ids is already formated with special tokens for the model.\"\r\n","                )\r\n","            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\r\n","\r\n","        if token_ids_1 is not None:\r\n","            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\r\n","        return [1] + ([0] * len(token_ids_0)) + [1]\r\n","\r\n","    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\r\n","        \"\"\"\r\n","        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\r\n","        A KoBERT sequence pair mask has the following format:\r\n","        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\r\n","        | first sequence    | second sequence\r\n","        if token_ids_1 is None, only returns the first portion of the mask (0's).\r\n","        \"\"\"\r\n","        sep = [self.sep_token_id]\r\n","        cls = [self.cls_token_id]\r\n","        if token_ids_1 is None:\r\n","            return len(cls + token_ids_0 + sep) * [0]\r\n","        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\r\n","\r\n","    def save_vocabulary(self, save_directory):\r\n","        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\r\n","            to a directory.\r\n","        \"\"\"\r\n","        if not os.path.isdir(save_directory):\r\n","            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\r\n","            return\r\n","\r\n","        # 1. Save sentencepiece model\r\n","        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\r\n","\r\n","        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\r\n","            copyfile(self.vocab_file, out_vocab_model)\r\n","\r\n","        # 2. Save vocab.txt\r\n","        index = 0\r\n","        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\r\n","        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\r\n","            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\r\n","                if index != token_index:\r\n","                    logger.warning(\r\n","                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\r\n","                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\r\n","                    )\r\n","                    index = token_index\r\n","                writer.write(token + \"\\n\")\r\n","                index += 1\r\n","\r\n","        return out_vocab_model, out_vocab_txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aa-K2LFwnEtC"},"source":["# BERT tokenizer\r\n","tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p5xXID_UnEkl"},"source":["# 테스트셋을 위한 함수들. info 컬럼이 없기 때문에 트레인셋과 살짝 다름(그냥 label관련 내용만 삭제)\r\n","def predict_convert_data(data_df,DATA_COLUMN):\r\n","    global tokenizer\r\n","    tokens, masks, segments = [], [], []\r\n","    \r\n","    for i in tqdm(range(len(data_df))):\r\n","\r\n","        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\r\n","        num_zeros = token.count(0)\r\n","        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\r\n","        segment = [0]*SEQ_LEN\r\n","\r\n","        tokens.append(token)\r\n","        segments.append(segment)\r\n","        masks.append(mask)\r\n","\r\n","    tokens = np.array(tokens)\r\n","    masks = np.array(masks)\r\n","    segments = np.array(segments)\r\n","    return [tokens, masks, segments]\r\n","\r\n","# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\r\n","def predict_load_data(pandas_dataframe,DATA_COLUMN):\r\n","    data_df = pandas_dataframe\r\n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\r\n","    data_x = predict_convert_data(data_df,DATA_COLUMN)\r\n","    return data_x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mLfh_FLznEhv"},"source":["test_ti = predict_load_data(test,'title')\r\n","test_co = predict_load_data(test,'content')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsODPzV3Xk1c"},"source":["def create_bert():\r\n","\r\n","    # 버트 pretrained 모델 로드\r\n","    model1 = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\r\n","    model2 = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\r\n","\r\n","    # 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의1\r\n","    token_inputs1 = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids1')\r\n","    mask_inputs1 = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks1')\r\n","    segment_inputs1 = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment1')\r\n","\r\n","    # 토큰 인풋, 마스크 인풋, 세그먼트 인풋 정의2\r\n","    token_inputs2 = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids2')\r\n","    mask_inputs2 = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks2')\r\n","    segment_inputs2 = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment2')\r\n","\r\n","    # 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의1\r\n","    bert_outputs1 = model1([token_inputs1, mask_inputs1, segment_inputs1])\r\n","    bert_outputs1 = bert_outputs1[1]\r\n","    \r\n","    # 인풋이 [토큰, 마스크, 세그먼트]인 모델 정의2\r\n","    bert_outputs2 = model2([token_inputs2, mask_inputs2, segment_inputs2])\r\n","    bert_outputs2 = bert_outputs2[1]\r\n","\r\n","    # concatenate\r\n","    bert_outputs = tf.keras.layers.concatenate([bert_outputs1, bert_outputs2])\r\n","    mid_layer = tf.keras.layers.Dense(256, activation='relu', kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02))(bert_outputs)\r\n","    mid_layer2 = tf.keras.layers.Dropout(rate=0.2)(mid_layer)\r\n","    sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02))(mid_layer2)\r\n","\r\n","    sentiment_model = tf.keras.Model([token_inputs1, mask_inputs1, segment_inputs1, token_inputs2, mask_inputs2, segment_inputs2], sentiment_first)\r\n","    # 옵티마이저는 간단하게 Adam 옵티마이저 활용\r\n","    sentiment_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.00001), loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\r\n","    return sentiment_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"acHU9BlznEe9"},"source":["model = create_bert()\r\n","model.load_weights(\"bert.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gw5QmWoO0Dmf"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8Km8gKunEcU"},"source":["%%time\r\n","test_pred = model.predict([test_ti,test_co])\r\n","pred = np.round(test_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wiouGurAnEZs"},"source":["sample_submission['info'] = pred\r\n","sample_submission.to_csv(\"제출.csv\", index=False)\r\n","sample_submission"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2nE-gobYYuj"},"source":["print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbWdTyakZD_q"},"source":[""],"execution_count":null,"outputs":[]}]}